{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "sequence_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woneuy01/NLP_Jon/blob/master/sequence_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxwDI52sTe1j",
        "colab_type": "text"
      },
      "source": [
        "# Sequence Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KkB3RZITe1k",
        "colab_type": "text"
      },
      "source": [
        "Using an [example taken directly from The TensorFlow Authors](https://nbviewer.jupyter.org/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb), in this notebook we generate novel sequences of text based on a corpus of training data consisting of Shakespeare's complete works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc_wUKGuTe1l",
        "colab_type": "text"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvNMp5qtTe1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdluD4m0Te1o",
        "colab_type": "text"
      },
      "source": [
        "#### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln4ALyaHTe1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc2vrx6BTe1r",
        "colab_type": "text"
      },
      "source": [
        "#### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_dRDsO5Te1s",
        "colab_type": "code",
        "outputId": "4b11229a-9a44-4df7-9929-6ad8de83785c",
        "colab": {}
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL16DyS9Te1u",
        "colab_type": "code",
        "outputId": "9f6255e5-f77c-4b25-a4c3-3dd8da0c23bc",
        "colab": {}
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xzd-lstTe1x",
        "colab_type": "code",
        "outputId": "a587d4d5-2d06-42d7-9410-25dc471546b0",
        "colab": {}
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxXpOltFTe1z",
        "colab_type": "text"
      },
      "source": [
        "#### Convert characters to numerical index representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtmIE0_rTe1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc7Cx8nLTe12",
        "colab_type": "code",
        "outputId": "97a48af4-0900-4062-d594-41d6ba41fc68",
        "colab": {}
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUdO74jqTe14",
        "colab_type": "code",
        "outputId": "8cca53df-919d-4417-ad39-9c5bd3877287",
        "colab": {}
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbkWUEYbTe16",
        "colab_type": "text"
      },
      "source": [
        "#### Create training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnBkEWsiTe17",
        "colab_type": "code",
        "outputId": "e4de40d7-52ec-4aee-803f-1637a41af4f4",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create stream of characters for training dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxjYZerDTe18",
        "colab_type": "code",
        "outputId": "19adc571-4a11-4d55-df47-d3c0fb17a003",
        "colab": {}
      },
      "source": [
        "# Each sequence has 101 characters; we'll predict next char given the preceding ones, up to 100 chars of input\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) \n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSkL-_4ATe1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C94VsD8WTe2A",
        "colab_type": "code",
        "outputId": "4780f7f2-17c9-4319-c6e5-d23256653f1b",
        "colab": {}
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skh-zKq2Te2C",
        "colab_type": "code",
        "outputId": "abb0f9b8-c897-4b94-ac74-a8f5c034e772",
        "colab": {}
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpITkUIDTe2F",
        "colab_type": "text"
      },
      "source": [
        "#### Shuffle data for stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH_ED6t3Te2G",
        "colab_type": "code",
        "outputId": "22ca243c-8e42-4f6f-e7ca-4ccf6d6f5583",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijc7yG8TTe2I",
        "colab_type": "text"
      },
      "source": [
        "#### Design neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzycGkyLTe2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGWX3PBhTe2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True, # last state for index i in batch is used as initial state in next batch\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ]) # we put all models in a list\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZAPoU3lTe2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyBNUdf0Te2O",
        "colab_type": "code",
        "outputId": "260e3c3e-16f7-4d64-b1aa-0fa632246ea0",
        "colab": {}
      },
      "source": [
        "# Examine the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmGscMLYTe2Q",
        "colab_type": "code",
        "outputId": "5144f793-dc93-40d6-d298-42fb11ce8aae",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4fsvgstTe2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkyMJ4fITe2U",
        "colab_type": "code",
        "outputId": "fa2719af-4a2e-4753-87b8-bf78868a55c0",
        "colab": {}
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([41,  5, 11, 31, 38, 47, 14, 15, 56, 11,  2, 16, 61, 11, 34, 49,  0,\n",
              "       56, 35, 57, 17,  7, 48, 51, 42, 49, 17, 25, 48, 52, 50, 41,  8, 60,\n",
              "        0, 33, 47, 36, 42,  2, 53, 44,  5,  5, 52, 18, 34,  4, 20,  7, 16,\n",
              "       64, 57, 22, 13, 32, 13, 24, 32, 51,  2, 11, 60, 13, 58, 51, 26, 20,\n",
              "       14, 40, 59, 48, 47, 37, 50,  0, 39,  6, 63, 52, 36, 47,  4, 30, 33,\n",
              "       45, 14, 15, 58, 13, 30, 53, 26, 10, 48, 25, 30, 50, 29, 59])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFVEWQGJTe2W",
        "colab_type": "code",
        "outputId": "ee5ffccc-c374-4a44-9058-106d9d7a19fd",
        "colab": {}
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"st I did embrace him: yet his nature\\nIn that's no changeling; and I must excuse\\nWhat cannot be amend\"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"c';SZiBCr;!Dw;Vk\\nrWsE-jmdkEMjnlc.v\\nUiXd!of''nFV&H-DzsJATALTm!;vAtmNHBbujiYl\\na,ynXi&RUgBCtARoN:jMRlQu\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCT64lI8Te2Y",
        "colab_type": "text"
      },
      "source": [
        "#### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWkxXU80Te2Y",
        "colab_type": "code",
        "outputId": "a1afe50e-140f-4463-b239-6acf882829bb",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.173575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbXUe-k2Te2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aind_adITe2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './model_output/seqGen'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an_8OLKMTe2e",
        "colab_type": "text"
      },
      "source": [
        "#### Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md8-k4UTTe2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFF88zuxTe2f",
        "colab_type": "code",
        "outputId": "a5d1214d-901e-4fb0-b7ff-455b0f626142",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 152s 882ms/step - loss: 2.6992\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 150s 874ms/step - loss: 1.9659\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 150s 874ms/step - loss: 1.6982\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 150s 871ms/step - loss: 1.5480\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 150s 872ms/step - loss: 1.4593\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 150s 871ms/step - loss: 1.3992\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 150s 870ms/step - loss: 1.3532\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 149s 868ms/step - loss: 1.3142\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 1.2783\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 1.2462\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 1.2139\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 1.1815\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 1.1491\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 1.1169\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 1.0817\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 1.0468\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 149s 867ms/step - loss: 1.0115\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 149s 868ms/step - loss: 0.9757\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 0.9396\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 149s 867ms/step - loss: 0.9051\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 0.8733\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 0.8439\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 0.8167\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 0.7920\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 0.7714\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 0.7529\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 0.7334\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 149s 866ms/step - loss: 0.7189\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 149s 865ms/step - loss: 0.7056\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 149s 867ms/step - loss: 0.6955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlnYwvGMTe2h",
        "colab_type": "text"
      },
      "source": [
        "#### Rebuild model with single output for generating text one char at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ashSj4gsTe2h",
        "colab_type": "code",
        "outputId": "3cb61131-2680-4e79-b9bd-8be77f88aa4c",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./model_output/seqGen/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buatHxAbTe2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PWUkQ_dTe2k",
        "colab_type": "code",
        "outputId": "357f16a1-dc8b-47fd-83de-7cc704d33ac7",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dewer3tSTe2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the char returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted char as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDoXlK8XTe2p",
        "colab_type": "code",
        "outputId": "56adda95-4623-4c64-be63-d4417d8846a6",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: O, the news I'll be at dinner:\n",
            "Sirrah, I pray how o'er-run he madest.\n",
            "\n",
            "BRUTUS:\n",
            "Then make you assist me, thou never trust\n",
            "They'll take it off. Camillo,\n",
            "And say yon our general, break out,\n",
            "With raging winter, at our part, have taken\n",
            "As shall deliver you.\n",
            "\n",
            "CORIOLANUS:\n",
            "Whoever once what then?\n",
            "Have write in Saying, ho!\n",
            "\n",
            "PETRUCHIO:\n",
            "What, will you hear?\n",
            "Why dost thou slip, nor now, what news?\n",
            "\n",
            "BUSHY:\n",
            "O, call them forgive; he's all deed sense you talk of death,\n",
            "If whit is head, then he corrects contented: speak, worthy sir\n",
            "Cisizenant comes our old and my life.\n",
            "\n",
            "BENVOLIO:\n",
            "A poor Hastings, people; peace,\n",
            "Nurreverly look, behold, why she's a little thing.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "For us, but than your eyes great dispersed,\n",
            "Most bone, my answord' with rats that yet this co fingers, to your\n",
            "wrongs to the people.\n",
            "\n",
            "CORIOLANUS:\n",
            "O villain, my tender barred chances\n",
            "Our subjects, assistant and thee.\n",
            "\n",
            "Second Senator:\n",
            "So. they be not tongue-tied:\n",
            "Or if wary barcal grant. They had guiltie is not so.\n",
            "\n",
            "GLOUCESTER:\n",
            "M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ln10nxMTe2r",
        "colab_type": "code",
        "outputId": "b6fd8fd0-ed40-41af-ffa5-fc69955dd8a9",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u\"HAMLET: To be, or not to be: that is the question:\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HAMLET: To be, or not to be: that is the question: when you\n",
            "called, better.\n",
            "\n",
            "POMPEY:\n",
            "Very well, then,\n",
            "But now 'tis pardon'd in the head of Milan are o\n",
            "That I have begun,\n",
            "And let his tribunes at the behalf conquered:\n",
            "'llays us here anon.\n",
            "Cousin of Buckingham?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Most noble sir,\n",
            "Le does remain in arms,\n",
            "And stops my tale abadem.\n",
            "\n",
            "MENENIUS:\n",
            "Here, it;\n",
            "Of the people, poor Milo!\n",
            "\n",
            "DUKE OF YORK:\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What stoom me? They are not simple Henry,\n",
            "Our trusty servants, stards, the sun comes\n",
            "Some remotion.\n",
            "\n",
            "LUCIO:\n",
            "I warrant you, I must obey?\n",
            "\n",
            "SAMPSON:\n",
            "Fear\n",
            "They ran a traitor, never bearing\n",
            "married in evy\n",
            "Know my daughter nor the time begins. Turns residence\n",
            "come; and would prove unpurped thee take this vestard by\n",
            "t.\n",
            "Well they are but people,\n",
            "He had thus stoop will be heredonable?\n",
            "I will to her married to at the rest;\n",
            "King Edward, Capulet'st; one one, it is request,\n",
            "And then dwn you were dead! what then?\n",
            "\n",
            "First Citizen:\n",
            "We are blessed with the earth away.\n",
            "\n",
            "BAPTISTA:\n",
            "So shall the deputy.\n",
            "\n",
            "PETRUCHIO:\n",
            "Come, but that one of those I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmalbrVTe2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}